{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymptotic Notation\n",
    "\n",
    "First, let's review some asymptotic notation.  We'll see it a fair amount in this course.\n",
    "\n",
    "Let $f(x), g(x)$ be some functions.  We say\n",
    "$$f = O(g)$$\n",
    "as $x\\to \\infty$ if there exists some constant $c\\ge 0$ and $x_0\\ge 0$ so that $ |f(x)| \\le c g(x)$ for all $x \\ge x_0$.\n",
    "\n",
    "We say\n",
    "$$f = \\Omega(g)$$\n",
    "as $x\\to \\infty$ if there exists some constant $c\\ge 0$ and $x_0\\ge 0$ so that $ |f(x)| \\ge c g(x)$ for all $x \\ge x_0$.\n",
    "\n",
    "Finally,\n",
    "$$f = \\Theta(g)$$\n",
    "if $f = O(g)$ and $f = \\Omega(g)$ (some constant multiples of $g$ bound $f$ above and below).\n",
    "\n",
    "Often we're interested in bounding worst-case behavior.  This means you'll see a lot of $O$, and less of $\\Omega$ and $\\Theta$.\n",
    "\n",
    "You may also see little-o notation such as $f = o(g)$, or the corresponding $f = \\omega(g)$.  Intuitively, $f = o(g)$ means $g$ grows much faster than $f$, i.e. $\\frac{\\vert f(x)\\vert}{g(x)}\\rightarrow 0$ as $x \\rightarrow \\infty$.  If $f = O(g)$ it can grow at the same rate, but if $f = o(g)$, it grows at a slower rate.\n",
    "\n",
    "Note that because we can scale by constant multiples, we typically drop them.\n",
    "1. $O(5x) = 5x = O(x)$\n",
    "2. $O(c) = O(1)$\n",
    "3. $O(c f(x)) = O(f(x))$\n",
    "\n",
    "## Examples\n",
    "\n",
    "1. $x = O(x)$\n",
    "2. $x = O(x^2)$\n",
    "3. $x^a = O(x^b)$ for $a \\le b$\n",
    "4. $x^a = O(b^x)$ for any $a, b > 1$\n",
    "5. $f = O(g)$ and $g = O(h)$ implies $f = O(h)$ (exercise: prove this)\n",
    "6. $f = O(g)$ and $h = O(k)$ implies $f\\times h = O(g \\times k)$ (exercise: prove this as well)\n",
    "\n",
    "## Answers to example 5 and 6\n",
    "\n",
    "Answer to example 5: \n",
    "\n",
    "By definition, since $f = O(g)$, we know there exists some constant $p\\ge 0$ and $x_0\\ge 0$ so that $ |f(x)| \\le p g(x)$ for all $x \\ge x_0$; since $g = O(h)$, we know there exists some constant $q\\ge 0$ and $x_1\\ge 0$ so that $ |g(x)| \\le q h(x)$ for all $x \\ge x_1$. Therefore, we get $ |f(x)| \\le p g(x)$ $ \\le (p*q) h(x)$ as $x \\ge $ $max$ {$x_0$, $x_1$}, which implies $f = O(h)$.\n",
    "\n",
    "Answer to example 6:\n",
    "Still, by definition, since $f = O(g)$, we know there exists some constant $p\\ge 0$ and $x_0\\ge 0$ so that $ |f(x)| \\le p g(x)$ for all $x \\ge x_0$; since $h = O(k)$, we know there exists some constant $q\\ge 0$ and $x_1\\ge 0$ so that $ |h(x)| \\le q k(x)$ for all $x \\ge x_1$. Now, take $x'$ = $max$ {$x_0$, $x_1$}, we know as $x\\ge x'$, we have both $ |f(x)| \\le p g(x)$ and $ |h(x)| \\le q k(x)$. Therefore, now we have: $|(f\\times h) (x)| \\le |f(x)| * |h(x)| \\le (p*q) g(x) k(x) = (p*q) (g\\times k) (x)$, proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "When we talk about [computational complexity](https://en.wikipedia.org/wiki/Computational_complexity), we are typically talking about the time or memory resources needed for an algorithm. \n",
    "\n",
    "For example, if an algorithm operates on an array of $n$ elements, we say the algorithm runs in $O(n^2)$ time if the run time scales (at worst) quadratically with the size $n$.\n",
    "\n",
    "For example, consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def maximum(x):\n",
    "    \"\"\"\n",
    "    returns the maximum value in an array x\n",
    "    \"\"\"\n",
    "    xmax = -np.infty\n",
    "    for xi in x:\n",
    "        if xi > xmax:\n",
    "            xmax = xi\n",
    "            \n",
    "    return xmax\n",
    "\n",
    "maximum([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our `maximum` function loops over the list once, and at each step of the iteration we do a constant amount of work. If `maximum` takes in an array of length `n` as input, this means there are `n` iterations of the for-loop, so `maximum` takes $O(n)$ time to compute.  Note that we don't need to create any additional arrays, so we can also say `maximum` uses $O(1)$ (constant) extra space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "ns = [10**k for k in range(1,8)]\n",
    "ts = []\n",
    "for n in ns:\n",
    "    a = np.random.rand(n)\n",
    "    start = time()\n",
    "    amax = maximum(a)\n",
    "    end = time()\n",
    "    ts.append(end - start)\n",
    "    \n",
    "plt.loglog(ns, ts)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('time (sec.)')\n",
    "plt.title('time to compute maximum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting functions with polynomial complexity ($O(x^a)$ for some $a$), it is standard to use a `loglog` plot.\n",
    "\n",
    "Why?  If $t(n) = \\Theta(n^a)$, then \n",
    "\\begin{align*}\n",
    "t(n) &\\sim   c n^a\\\\\n",
    "\\log t(n) &\\sim a \\log n + \\log c\n",
    "\\end{align*}\n",
    "\n",
    "I.e. the polynomial exponent is the slope of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = np.log(ns)\n",
    "lt = np.log(ts)\n",
    "coeff = np.polyfit(ln, lt, 1) # fit a polynomial\n",
    "coeff[0] # highest degree (linear) term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_subarrays(x):\n",
    "    \"\"\"\n",
    "    return all sub-arrays of x\n",
    "    \"\"\"\n",
    "    if len(x) == 1:\n",
    "        return [x, []]\n",
    "    else:\n",
    "        ret = []\n",
    "        subs = all_subarrays(x[:-1]) # subarrays with all but the last element\n",
    "        ret.extend(subs) # all subarrrays without the last element\n",
    "        for s in subs:\n",
    "            scpy = s.copy()\n",
    "            scpy.append(x[-1])\n",
    "            ret.append(scpy)\n",
    "        return ret\n",
    "        \n",
    "all_subarrays([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the length of output of `all_subarrays` grows as $2^n$, where `n = len(x)`.  This is a good hint that the function has exponential time complexity and space complexity $O(2^n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = range(1, 20)\n",
    "ts = []\n",
    "for n in ns:\n",
    "    a = [i for i in range(n)]\n",
    "    start = time()\n",
    "    ret = all_subarrays(a)\n",
    "    end = time()\n",
    "    ts.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it is better to use a `semilogy` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(ns, ts)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('time (sec.)')\n",
    "plt.title('time to form all subarrays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Why does a semilogy plot make sense for plotting the time it takes to run a function with exponential time complexity?  \n",
    "\n",
    "How can you interpret the expected slope?\n",
    "\n",
    "## Answer to the exercise:\n",
    "\n",
    "$y = \\log(2^n) = \\log(2)n$. For a function with exponential time complexity, if we do not use semilogy plot, the values on the y-axis could be distributed in a large range. Also, using semilogy plot could provide us a nicer visualization since linear-like pattern could be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycourse)",
   "language": "python",
   "name": "pycourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
